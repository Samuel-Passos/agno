"""üé® Gerador de Post de Blog v2.0 - Seu Est√∫dio de Cria√ß√£o de Conte√∫do de IA!

Este exemplo avan√ßado demonstra como construir um gerador sofisticado de posts de blog usando
a nova arquitetura de workflow v2.0. O workflow combina capacidades de pesquisa web com
expertise profissional de escrita usando uma abordagem multi-est√°gio:

1. Pesquisa web inteligente e coleta de fontes
2. Extra√ß√£o e processamento de conte√∫do
3. Escrita profissional de post de blog com cita√ß√µes adequadas

Capacidades principais:
- Pesquisa web avan√ßada e avalia√ß√£o de fontes
- Raspagem e processamento de conte√∫do
- Escrita profissional com otimiza√ß√£o SEO
- Cache autom√°tico de conte√∫do para efici√™ncia
- Atribui√ß√£o de fontes e verifica√ß√£o de fatos
"""

import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field


# --- Modelos de Resposta ---
class NewsArticle(BaseModel):
    title: str = Field(..., description="T√≠tulo do artigo.")
    url: str = Field(..., description="Link para o artigo.")
    summary: Optional[str] = Field(
        ..., description="Resumo do artigo se dispon√≠vel."
    )


class SearchResults(BaseModel):
    articles: list[NewsArticle]


class ScrapedArticle(BaseModel):
    title: str = Field(..., description="T√≠tulo do artigo.")
    url: str = Field(..., description="Link para o artigo.")
    summary: Optional[str] = Field(
        ..., description="Resumo do artigo se dispon√≠vel."
    )
    content: Optional[str] = Field(
        ...,
        description="Conte√∫do completo do artigo em formato markdown. None se o conte√∫do n√£o estiver dispon√≠vel.",
    )


# --- Agentes ---
research_agent = Agent(
    name="Blog Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    description=dedent("""\
    Voc√™ √© BlogResearch-X, um assistente de pesquisa de elite especializado em descobrir
    fontes de alta qualidade para conte√∫do de blog convincente. Sua expertise inclui:

    - Encontrar fontes autorizadas e em tend√™ncia
    - Avaliar credibilidade e relev√¢ncia do conte√∫do
    - Identificar perspectivas diversas e opini√µes de especialistas
    - Descobrir √¢ngulos √∫nicos e insights
    - Garantir cobertura abrangente do t√≥pico
    """),
    instructions=dedent("""\
    1. Estrat√©gia de Busca üîç
       - Encontrar 10-15 fontes relevantes e selecionar as 5-7 melhores
       - Priorizar conte√∫do recente e autorizado
       - Procurar √¢ngulos √∫nicos e insights de especialistas
    2. Avalia√ß√£o de Fontes üìä
       - Verificar credibilidade e expertise da fonte
       - Verificar datas de publica√ß√£o para atualidade
       - Avaliar profundidade e singularidade do conte√∫do
    3. Diversidade de Perspectivas üåê
       - Incluir diferentes pontos de vista
       - Coletar opini√µes tanto mainstream quanto de especialistas
       - Encontrar dados e estat√≠sticas de apoio
    """),
    output_schema=SearchResults,
)

content_scraper_agent = Agent(
    name="Content Scraper Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[Newspaper4kTools()],
    description=dedent("""\
    Voc√™ √© ContentBot-X, um especialista em extrair e processar conte√∫do digital
    para cria√ß√£o de blog. Sua expertise inclui:

    - Extra√ß√£o eficiente de conte√∫do
    - Formata√ß√£o e estrutura√ß√£o inteligente
    - Identifica√ß√£o de informa√ß√µes-chave
    - Preserva√ß√£o de cita√ß√µes e estat√≠sticas
    - Manuten√ß√£o de atribui√ß√£o de fontes
    """),
    instructions=dedent("""\
    1. Extra√ß√£o de Conte√∫do üìë
       - Extrair conte√∫do do artigo
       - Preservar cita√ß√µes e estat√≠sticas importantes
       - Manter atribui√ß√£o adequada
       - Lidar com paywalls graciosamente
    2. Processamento de Conte√∫do üîÑ
       - Formatar texto em markdown limpo
       - Preservar informa√ß√µes-chave
       - Estruturar conte√∫do logicamente
    3. Controle de Qualidade ‚úÖ
       - Verificar relev√¢ncia do conte√∫do
       - Garantir extra√ß√£o precisa
       - Manter legibilidade
    """),
    output_schema=ScrapedArticle,
)

blog_writer_agent = Agent(
    name="Blog Writer Agent",
    model=OpenAIChat(id="gpt-4o"),
    description=dedent("""\
    Voc√™ √© BlogMaster-X, um criador de conte√∫do de elite combinando excel√™ncia jornal√≠stica
    com expertise em marketing digital. Seus pontos fortes incluem:

    - Criar manchetes dignas de viral
    - Escrever introdu√ß√µes envolventes
    - Estruturar conte√∫do para consumo digital
    - Incorporar pesquisa perfeitamente
    - Otimizar para SEO mantendo qualidade
    - Criar conclus√µes compartilh√°veis
    """),
    instructions=dedent("""\
    1. Estrat√©gia de Conte√∫do üìù
       - Criar manchetes que chamam aten√ß√£o
       - Escrever introdu√ß√µes convincentes
       - Estruturar conte√∫do para engajamento
       - Incluir subt√≠tulos relevantes
    2. Excel√™ncia na Escrita ‚úçÔ∏è
       - Equilibrar expertise com acessibilidade
       - Usar linguagem clara e envolvente
       - Incluir exemplos relevantes
       - Incorporar estat√≠sticas naturalmente
    3. Integra√ß√£o de Fontes üîç
       - Citar fontes adequadamente
       - Incluir cita√ß√µes de especialistas
       - Manter precis√£o factual
    4. Otimiza√ß√£o Digital üíª
       - Estruturar para escaneabilidade
       - Incluir takeaways compartilh√°veis
       - Otimizar para SEO
       - Adicionar subt√≠tulos envolventes

    Formatar seu post de blog com esta estrutura:
    # {Manchete Digna de Viral}

    ## Introdu√ß√£o
    {Gancho envolvente e contexto}

    ## {Se√ß√£o Convincente 1}
    {Insights-chave e an√°lise}
    {Cita√ß√µes de especialistas e estat√≠sticas}

    ## {Se√ß√£o Envolvente 2}
    {Explora√ß√£o mais profunda}
    {Exemplos do mundo real}

    ## {Se√ß√£o Pr√°tica 3}
    {Insights acion√°veis}
    {Recomenda√ß√µes de especialistas}

    ## Principais Takeaways
    - {Insight compartilh√°vel 1}
    - {Takeaway pr√°tico 2}
    - {Achado not√°vel 3}

    ## Fontes
    {Fontes adequadamente atribu√≠das com links}
    """),
    markdown=True,
)


# --- Fun√ß√µes Auxiliares ---
def get_cached_blog_post(session_state, topic: str) -> Optional[str]:
    """Obter post de blog em cache do estado da sess√£o do workflow"""
    logger.info("Verificando se existe post de blog em cache")
    return session_state.get("blog_posts", {}).get(topic)


def cache_blog_post(session_state, topic: str, blog_post: str):
    """Armazenar post de blog em cache no estado da sess√£o do workflow"""
    logger.info(f"Salvando post de blog para t√≥pico: {topic}")
    if "blog_posts" not in session_state:
        session_state["blog_posts"] = {}
    session_state["blog_posts"][topic] = blog_post


def get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:
    """Obter resultados de busca em cache do estado da sess√£o do workflow"""
    logger.info("Verificando se existem resultados de busca em cache")
    search_results = session_state.get("search_results", {}).get(topic)
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"N√£o foi poss√≠vel validar resultados de busca em cache: {e}")
    return search_results if isinstance(search_results, SearchResults) else None


def cache_search_results(session_state, topic: str, search_results: SearchResults):
    """Armazenar resultados de busca em cache no estado da sess√£o do workflow"""
    logger.info(f"Salvando resultados de busca para t√≥pico: {topic}")
    if "search_results" not in session_state:
        session_state["search_results"] = {}
    session_state["search_results"][topic] = search_results.model_dump()


def get_cached_scraped_articles(
    session_state, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """Obter artigos raspados em cache do estado da sess√£o do workflow"""
    logger.info("Verificando se existem artigos raspados em cache")
    scraped_articles = session_state.get("scraped_articles", {}).get(topic)
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"N√£o foi poss√≠vel validar artigos raspados em cache: {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None


def cache_scraped_articles(
    session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """Armazenar artigos raspados em cache no estado da sess√£o do workflow"""
    logger.info(f"Salvando artigos raspados para t√≥pico: {topic}")
    if "scraped_articles" not in session_state:
        session_state["scraped_articles"] = {}
    session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }


async def get_search_results(
    session_state, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """Obter resultados de busca com suporte a cache"""

    # Verificar cache primeiro
    if use_cache:
        cached_results = get_cached_search_results(session_state, topic)
        if cached_results:
            logger.info(f"Encontrados {len(cached_results.articles)} artigos em cache.")
            return cached_results

    # Buscar novos resultados
    for attempt in range(num_attempts):
        try:
            print(
                f"üîç Pesquisando artigos sobre: {topic} (tentativa {attempt + 1}/{num_attempts})"
            )
            response = await research_agent.arun(topic)

            if (
                response
                and response.content
                and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"Encontrados {article_count} artigos na tentativa {attempt + 1}")
                print(f"‚úÖ Encontrados {article_count} artigos relevantes")

                # Armazenar resultados em cache
                cache_search_results(session_state, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"Tentativa {attempt + 1}/{num_attempts} falhou: Tipo de resposta inv√°lido"
                )

        except Exception as e:
            logger.warning(f"Tentativa {attempt + 1}/{num_attempts} falhou: {str(e)}")

    logger.error(f"Falha ao obter resultados de busca ap√≥s {num_attempts} tentativas")
    return None


async def scrape_articles(
    session_state,
    topic: str,
    search_results: SearchResults,
    use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """Raspar artigos com suporte a cache"""

    # Verificar cache primeiro
    if use_cache:
        cached_articles = get_cached_scraped_articles(session_state, topic)
        if cached_articles:
            logger.info(f"Encontrados {len(cached_articles)} artigos raspados em cache.")
            return cached_articles

    scraped_articles: Dict[str, ScrapedArticle] = {}

    print(f"üìÑ Raspando {len(search_results.articles)} artigos...")

    for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f"üìñ Raspando artigo {i}/{len(search_results.articles)}: {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

            if (
                response
                and response.content
                and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"Artigo raspado: {response.content.url}")
                print(f"‚úÖ Raspado com sucesso: {response.content.title[:50]}...")
            else:
                print(f"‚ùå Falha ao raspar: {article.title[:50]}...")

        except Exception as e:
            logger.warning(f"Falha ao raspar {article.url}: {str(e)}")
            print(f"‚ùå Erro ao raspar: {article.title[:50]}...")

    # Armazenar artigos raspados em cache
    cache_scraped_articles(session_state, topic, scraped_articles)
    return scraped_articles


# --- Fun√ß√£o Principal de Execu√ß√£o ---
async def blog_generation_execution(
    session_state,
    topic: str = None,
    use_search_cache: bool = True,
    use_scrape_cache: bool = True,
    use_blog_cache: bool = True,
) -> str:
    """
    Fun√ß√£o de execu√ß√£o do workflow de gera√ß√£o de post de blog.

    Args:
        session_state: O estado compartilhado da sess√£o
        topic: T√≥pico do post de blog (se n√£o fornecido, usa execution_input.input)
        use_search_cache: Se deve usar resultados de busca em cache
        use_scrape_cache: Se deve usar artigos raspados em cache
        use_blog_cache: Se deve usar posts de blog em cache
    """

    blog_topic = topic

    if not blog_topic:
        return "‚ùå Nenhum t√≥pico de blog fornecido. Por favor, especifique um t√≥pico."

    print(f"üé® Gerando post de blog sobre: {blog_topic}")
    print("=" * 60)

    # Verificar post de blog em cache primeiro
    if use_blog_cache:
        cached_blog = get_cached_blog_post(session_state, blog_topic)
        if cached_blog:
            print("üìã Post de blog em cache encontrado!")
            return cached_blog

    # Fase 1: Pesquisa e coleta de fontes
    print("\nüîç FASE 1: PESQUISA E COLETA DE FONTES")
    print("=" * 50)

    search_results = await get_search_results(
        session_state, blog_topic, use_search_cache
    )

    if not search_results or len(search_results.articles) == 0:
        return f"‚ùå Desculpe, n√£o foi poss√≠vel encontrar artigos sobre o t√≥pico: {blog_topic}"

    print(f"üìä Encontradas {len(search_results.articles)} fontes relevantes:")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

    # Fase 2: Extra√ß√£o de conte√∫do
    print("\nüìÑ FASE 2: EXTRA√á√ÉO DE CONTE√öDO")
    print("=" * 50)

    scraped_articles = await scrape_articles(
        session_state, blog_topic, search_results, use_scrape_cache
    )

    if not scraped_articles:
        return f"‚ùå N√£o foi poss√≠vel extrair conte√∫do de nenhum artigo para o t√≥pico: {blog_topic}"

    print(f"üìñ Conte√∫do extra√≠do com sucesso de {len(scraped_articles)} artigos")

    # Fase 3: Escrita do post de blog
    print("\n‚úçÔ∏è FASE 3: CRIA√á√ÉO DO POST DE BLOG")
    print("=" * 50)

    # Preparar entrada para o escritor
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

    print("ü§ñ IA est√° criando seu post de blog...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

    if not writer_response or not writer_response.content:
        return f"‚ùå Falha ao gerar post de blog para o t√≥pico: {blog_topic}"

    blog_post = writer_response.content

    # Armazenar post de blog em cache
    cache_blog_post(session_state, blog_topic, blog_post)

    print("‚úÖ Post de blog gerado com sucesso!")
    print(f"üìù Comprimento: {len(blog_post)} caracteres")
    print(f"üìö Fontes: {len(scraped_articles)} artigos")

    return blog_post


# --- Defini√ß√£o do Workflow ---
blog_generator_workflow = Workflow(
    name="Blog Post Generator",
    description="Gerador avan√ßado de posts de blog com capacidades de pesquisa e cria√ß√£o de conte√∫do",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/blog_generator.db",
    ),
    steps=blog_generation_execution,
    session_state={},  # Inicializar estado de sess√£o vazio para cache
)


if __name__ == "__main__":
    import random

    async def main():
        # T√≥picos de exemplo divertidos para mostrar a versatilidade do gerador
        example_topics = [
            "The Rise of Artificial General Intelligence: Latest Breakthroughs",
            "How Quantum Computing is Revolutionizing Cybersecurity",
            "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint",
            "The Future of Work: AI and Human Collaboration",
            "Space Tourism: From Science Fiction to Reality",
            "Mindfulness and Mental Health in the Digital Age",
            "The Evolution of Electric Vehicles: Current State and Future Trends",
            "Why Cats Secretly Run the Internet",
            "The Science Behind Why Pizza Tastes Better at 2 AM",
            "How Rubber Ducks Revolutionized Software Development",
        ]

        # Testar com um t√≥pico aleat√≥rio
        topic = random.choice(example_topics)

        print("üß™ Testando Gerador de Post de Blog v2.0")
        print("=" * 60)
        print(f"üìù T√≥pico: {topic}")
        print()

        # Generate the blog post
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

        pprint_run_response(resp, markdown=True, show_time=True)

    asyncio.run(main())
